{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ef15ae",
   "metadata": {},
   "source": [
    "## Prepare for Transfer Learning (Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "001b2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_BATCHES = 256\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(size=(64, 64)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomCrop(54),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], # ImageNet's data values used for the Pre-Trained_Model(ResNet50)\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(size=(64, 64)),\n",
    "        transforms.RandomCrop(54),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(size=(64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "base_dir = './splitted_data'\n",
    "\n",
    "img_datasets = {x: ImageFolder(root=os.path.join(base_dir, x),\n",
    "                                    transform=data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(dataset=img_datasets[x], \n",
    "                                              batch_size=NUM_BATCHES, num_workers=4, \n",
    "                                              shuffle=True) for x in ['train', 'val', 'test']}\n",
    "\n",
    "class_names = img_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91a2567",
   "metadata": {},
   "source": [
    "## Import the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac95515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa42e146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# import the pretrained ResNet50 and change the fc layer to the apppropriate output_features (34 classes)\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 34)\n",
    "resnet = resnet.to(DEVICE)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, resnet.parameters()),\n",
    "                             lr = 0.001)\n",
    "\n",
    "# lower the learning rate by multiplying 0.1 every 7 epochs\n",
    "res_lr_scheduler = lr_scheduler.StepLR(optimizer,\n",
    "                                       step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1b0e4c",
   "metadata": {},
   "source": [
    "## Freeze some parts of Pre-Trained Model's Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24000c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 0\n",
    "for child in resnet.children():\n",
    "    layer += 1\n",
    "    if layer < 6:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f9c12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "dataset_sizes = {x: len(img_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "\n",
    "def train_resnet(model: torch.nn.Module,\n",
    "                 loss_fn: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 scheduler,\n",
    "                 num_epochs: int = 25):\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"-------- epoch {epoch} --------\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for step in ['train', 'val']:\n",
    "            if step == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[step]:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(step == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "                    \n",
    "                    if step == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if step == 'train':\n",
    "                scheduler.step()\n",
    "                l_r = [x['lr'] for x in optimizer.param_groups]\n",
    "                print(\"learning rate: \", l_r)\n",
    "                \n",
    "            epoch_loss = running_loss/dataset_sizes[step]\n",
    "            epoch_acc = running_corrects.double()/dataset_sizes[step]\n",
    "            \n",
    "            print(f\"{step} Loss: {epoch_loss:.4f} | Acc: {epoch_acc}\")\n",
    "            \n",
    "            if step == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "        end_time = time.time()\n",
    "        time_elapsed = end_time - start_time\n",
    "        print(f\"Completed in {time_elapsed / 60:.0f}min {time_elapsed % 60:0f}s\")\n",
    "    print(f\"Best val Acc: {best_acc:.4f}\")\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72554a77",
   "metadata": {},
   "source": [
    "## Train the ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4822f845",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cebc5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 15 18:50:05 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 512.78       Driver Version: 512.78       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   40C    P8    N/A /  N/A |   1979MiB /  2048MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     30524      C   ...thon\\Python311\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67beb303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- epoch 0 --------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 0.5387 | Acc: 0.8432670190872039\n",
      "val Loss: 0.3344 | Acc: 0.9076699382042893\n",
      "Completed in 12min 26.150737s\n",
      "-------- epoch 1 --------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 0.1910 | Acc: 0.941769904362213\n",
      "val Loss: 0.1229 | Acc: 0.9601357082273113\n",
      "Completed in 13min 19.335580s\n",
      "-------- epoch 2 --------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 0.1393 | Acc: 0.9562164561559259\n",
      "val Loss: 0.1073 | Acc: 0.9635284139100933\n",
      "Completed in 13min 10.449545s\n",
      "-------- epoch 3 --------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 0.1141 | Acc: 0.9631976110729995\n",
      "val Loss: 0.0629 | Acc: 0.9804919423240034\n",
      "Completed in 12min 11.454653s\n",
      "-------- epoch 4 --------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 0.0903 | Acc: 0.9715104313788789\n",
      "val Loss: 0.0675 | Acc: 0.9778262450018175\n",
      "Completed in 12min 0.405803s\n",
      "-------- epoch 5 --------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 0.0743 | Acc: 0.9770388604172552\n",
      "val Loss: 0.0705 | Acc: 0.9778262450018175\n",
      "Completed in 13min 49.421884s\n",
      "-------- epoch 6 --------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.0765 | Acc: 0.9756668415318187\n",
      "val Loss: 0.0632 | Acc: 0.9803707742639041\n",
      "Completed in 12min 25.861896s\n",
      "-------- epoch 7 --------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.0354 | Acc: 0.9885396069569429\n",
      "val Loss: 0.0122 | Acc: 0.996607294317218\n",
      "Completed in 12min 27.360130s\n",
      "-------- epoch 8 --------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.0180 | Acc: 0.9942294499818409\n",
      "val Loss: 0.0089 | Acc: 0.9978189749182116\n",
      "Completed in 12min 23.487410s\n",
      "-------- epoch 9 --------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.0148 | Acc: 0.9956418223639079\n",
      "val Loss: 0.0080 | Acc: 0.9981824790985097\n",
      "Completed in 15min 37.738858s\n",
      "-------- epoch 10 --------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.0132 | Acc: 0.9958032363504298\n",
      "val Loss: 0.0070 | Acc: 0.9979401429783109\n",
      "Completed in 14min 48.856947s\n",
      "-------- epoch 11 --------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.0116 | Acc: 0.9960453573302127\n",
      "val Loss: 0.0061 | Acc: 0.9984248152187083\n",
      "Completed in 14min 53.184276s\n",
      "-------- epoch 12 --------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_resnet50 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_resnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_lr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model_resnet50, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet50.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[31], line 39\u001b[0m, in \u001b[0;36mtrain_resnet\u001b[1;34m(model, loss_fn, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 39\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     42\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_resnet50 = train_resnet(resnet, loss_fn, optimizer, res_lr_scheduler, num_epochs = 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be204dc",
   "metadata": {},
   "source": [
    "## Save the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f837187",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_resnet50' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel_resnet50\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet50.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_resnet50' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model_resnet50, 'resnet50.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff20cd0",
   "metadata": {},
   "source": [
    "## Evaluate Transfer Learning Model(ResNet50) on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34990ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: torch.nn.Module,\n",
    "             dataloader: torch.utils.data.DataLoader,\n",
    "             optimizer: torch.optim.Optimizer, \n",
    "             loss_fn: torch.nn.Module=nn.CrossEntropyLoss()):\n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    test_loss, correct, test_acc = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            # Forward pass\n",
    "            pred_probs = model(X)\n",
    "            # Accumulate the loss\n",
    "            test_loss += loss_fn(pred_probs, y).item()\n",
    "            \n",
    "            pred = torch.argmax(pred_probs, dim=1)\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc = 100. * correct / len(dataloader.dataset)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b16909e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resnet50' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresnet50\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      2\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m evaluate(resnet50, test_dataloader, optimizer, loss_fn)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet test acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'resnet50' is not defined"
     ]
    }
   ],
   "source": [
    "resnet50.eval()\n",
    "test_loss, test_acc = evaluate(resnet50, test_dataloader, optimizer, loss_fn)\n",
    "\n",
    "print(f\"ResNet test acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254241f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
